{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST-Fashion-GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMDz1KXYWkhVWDMup/XTskZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geoffcorvera/deeplearning-spring2021/blob/main/MNIST_Fashion_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsTTZ_GPh-77"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "from tensorflow.keras import layers\n",
        "from keras.datasets.fashion_mnist import load_data"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmRsdeio_ifo"
      },
      "source": [
        "# Define Generator Model\n",
        "\n",
        "\n",
        "*   Input: 100-dimensional sample from Gaussian latent space\n",
        "\n",
        "*   First dense layer: 100 neurons (followed by BatchNormalization & LeakyRELU)\n",
        "*   Intermediate layer: 500 dense (followed by BatchNormalization & LeakyRELU)\n",
        "*   Last dense layer: 784 (neurons) (sigmoid activation)\n",
        "*   Reshape into desired image shape \n",
        "\n",
        "*   Output: Generated image (28x28)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ_D_3RSNJwL"
      },
      "source": [
        "img_shape = (28,28,1)\n",
        "\n",
        "# TODO: Finesse the number of neurons for up/sampling\n",
        "# Input should have enough space for low-res version of generated image\n",
        "n_initial = 100\n",
        "n_intermediate = 500\n",
        "n_final = 784\n",
        "\n",
        "def makeDenseGenerator(latent_dims=100):\n",
        "  latent_sample = keras.Input(latent_dims)\n",
        "  # Input a sample from n-dimensional latent space (typically from Gaussian)\n",
        "  x = layers.Dense(n_initial)(latent_sample)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.LeakyReLU()(x)\n",
        "  # Upsample\n",
        "  x = layers.Dense(n_intermediate)(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.LeakyReLU()(x)\n",
        "  # Use sigmoid for bounded pixel intensities\n",
        "  pixel_intensities = layers.Dense(n_final, activation='sigmoid')(x)\n",
        "  # x = layers.BatchNormalization()(x)\n",
        "  # x = layers.LeakyReLU()(x)\n",
        "  # Reshape into image dimensions\n",
        "  generated_img = layers.Reshape(img_shape)(pixel_intensities)\n",
        "\n",
        "  generator = keras.Model(latent_sample, generated_img, name='generator')\n",
        "  generator.summary()\n",
        "\n",
        "  return generator\n",
        "\n",
        "\n",
        "# TODO: Use transpose convolution (+ bi-linear interp) for up-sampling?\n",
        "def createConvGenerator(latent_dims=100):\n",
        "  print('not implemented')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huofCCNPQgYC",
        "outputId": "9e7aadb2-2a27-468e-d9df-a586cfe681ed"
      },
      "source": [
        "g_model = makeDenseGenerator()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 100)]             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 500)               50500     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 784)               392784    \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 455,784\n",
            "Trainable params: 454,584\n",
            "Non-trainable params: 1,200\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJYoKMFfNLW5"
      },
      "source": [
        "# Define Descriminator Model\n",
        "\n",
        "Tries to mirror the generator network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-7VOXEDl0kw"
      },
      "source": [
        "def makeDiscriminator(input_shape=(28, 28, 1)):\n",
        "  input_img = keras.Input(input_shape)\n",
        "\n",
        "  x = layers.Flatten()(input_img)\n",
        "  x = layers.Dense(n_final)(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.LeakyReLU()(x)\n",
        "\n",
        "  x = layers.Dense(n_intermediate)(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.LeakyReLU()(x)\n",
        "\n",
        "  x = layers.Dense(n_initial)(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.LeakyReLU()(x)\n",
        "  \n",
        "  p_real = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "  discriminator = keras.Model(input_img, p_real, name='discriminator')\n",
        "\n",
        "  adam = keras.optimizers.Adam(learning_rate=.0002, beta_1=0.5)\n",
        "  discriminator.compile(\n",
        "    optimizer=adam,\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        "  )\n",
        "\n",
        "  discriminator.summary()\n",
        "\n",
        "  return discriminator  "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRw4xnPmQzMw",
        "outputId": "fd5eea8d-23a2-4a60-cafc-0f97c4654b5b"
      },
      "source": [
        "d_model = makeDiscriminator()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 784)               615440    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 784)               3136      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 500)               392500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 100)               50100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 1,063,677\n",
            "Trainable params: 1,060,909\n",
            "Non-trainable params: 2,768\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZIMpJ0mZa-Z"
      },
      "source": [
        "# Define GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "338_IpOn-5o1"
      },
      "source": [
        "def makeGAN(g_model, d_model, latent_dims=100):\n",
        "  d_model.trainable = False\n",
        "\n",
        "  latent_sample = keras.Input(shape=(latent_dims,1))\n",
        "  generated_img = g_model(latent_sample)\n",
        "  p_real = d_model(generated_img)\n",
        "  gan = keras.Model(latent_sample, p_real, name='gan')\n",
        "\n",
        "  adam = keras.optimizers.Adam(learning_rate=.0002, beta_1=0.5)\n",
        "  gan.compile(optimizer=adam, loss='binary_crossentropy')\n",
        "  gan.summary()\n",
        "\n",
        "  return gan"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cl9S5d4wvFl2"
      },
      "source": [
        "gan = makeGAN(g_model, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5snuxABfB-yS"
      },
      "source": [
        "# Load & Preprocess MNIST Fashion Dataset\n",
        "\n",
        "We load the training and test data as numpy arrays for the MNIST fashion dataset. The pixel intensities of the training and test images are rescaled to [0,1] by dividing intensities by max value (255). An extra dimension is added to each example, to specify image is single-channel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdMvFhERBWnR"
      },
      "source": [
        "def load_mnist():\n",
        "  (trainx, _), (testx, _) = load_data()\n",
        "\n",
        "  trainx = np.expand_dims(trainx, axis=-1)\n",
        "  testx = np.expand_dims(testx, axis=-1)\n",
        "\n",
        "  trainx = trainx / 255.\n",
        "  testx = testx / 255.\n",
        "\n",
        "  print('Train summary', trainx.shape, trainx.dtype)\n",
        "  print('Test summary', testx.shape, testx.dtype)\n",
        "\n",
        "  return trainx, testx"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAzTouollKky",
        "outputId": "fc66a3fd-aae1-4418-de9a-85b1490e438d"
      },
      "source": [
        "train_set, test_set = load_mnist()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train summary (60000, 28, 28, 1) float64\n",
            "Test summary (10000, 28, 28, 1) float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um-z-X0FUykT"
      },
      "source": [
        "# Generating Data for Training\n",
        "Our goal is to train the discriminator network to output the probability that an input image is real. To this end, we need to train the discriminator on a combination of real and fake/generated images. We give the real examples a target of 1, and fake/generated examples a target of 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkMAScO7UwtP"
      },
      "source": [
        "def generate_real_samples(dataset, n_samples):\n",
        "  # randomly sample examples from the dataset\n",
        "  indices = np.random.randint(0, len(dataset), n_samples),\n",
        "  samples = dataset[indices]\n",
        "  labels = np.ones((n_samples,1))\n",
        "\n",
        "  assert samples.shape[0] == labels.shape[0]\n",
        "  assert samples.shape[0] == n_samples\n",
        "  \n",
        "  return samples, labels\n",
        "\n",
        "# Make n_samples number of fake images (28x28)\n",
        "def generate_fake_samples(n_samples, g_model=None):\n",
        "  if (g_model):\n",
        "    z = generate_latent_points(n_samples)\n",
        "    X = g_model(z)\n",
        "  else:\n",
        "    X = np.random.rand(28*28*n_samples)\n",
        "    X = X.reshape((n_samples,28,28,1))\n",
        "\n",
        "  y = np.zeros((n_samples,1))\n",
        "  assert X.shape[0] == y.shape[0]\n",
        "\n",
        "  return X, y\n",
        "\n",
        "def generate_latent_points(n_samples, latent_dims=100):\n",
        "  X = np.random.rand(n_samples * latent_dims)\n",
        "  X = X.reshape(n_samples, latent_dims)\n",
        "  return X"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOz-7Wm1eXVR"
      },
      "source": [
        "# Training Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTz_4GXiDqzB"
      },
      "source": [
        "def train_discriminator(d_model, g_model, dataset, batch_size=32, n_iter=10):\n",
        "  for i in range(n_iter):\n",
        "    real_x, real_y = generate_real_samples(dataset, int(batch_size/2))\n",
        "    fake_x, fake_y = generate_fake_samples(int(batch_size/2), g_model)\n",
        "    real_acc = d_model.train_on_batch(real_x, real_y)\n",
        "    fake_acc = d_model.train_on_batch(fake_x, fake_y)\n",
        "\n",
        "# TODO: set epochs to 100\n",
        "def train(gan, g_model, d_model, dataset, batch_size=32, n_epochs=2):\n",
        "  n_batches = len(dataset) // batch_size\n",
        "  n_half = batch_size // 2\n",
        "\n",
        "  for i in range(n_epochs):\n",
        "    for b in range(n_batches):\n",
        "      x_fake, y_fake = generate_fake_samples(n_half, g_model)\n",
        "      x_real, y_real = generate_real_samples(dataset, n_half)\n",
        "      # Combine real and fake into single training batch\n",
        "      X = np.vstack([x_fake, x_real])\n",
        "      y = np.vstack([y_fake, y_real])\n",
        "\n",
        "      # shuffle batch\n",
        "      mask = np.arange(X.shape[0])\n",
        "      np.random.shuffle(mask)\n",
        "      X = X[mask]\n",
        "      y = y[mask]\n",
        "\n",
        "      loss_d = d_model.train_on_batch(X,y)\n",
        "      \n",
        "      x_gan = generate_latent_points(batch_size)\n",
        "      y_gan = np.ones((batch_size,1))\n",
        "      loss_gan = gan.train_on_batch(x_gan, y_gan)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYg8TRFAHeA5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "100bae91-e1eb-48cd-e0e2-b23bedd66e9d"
      },
      "source": [
        "train(gan, g_model, d_model, train_set)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_9 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 784)               615440    \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 784)               3136      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_15 (LeakyReLU)   (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 500)               392500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_16 (LeakyReLU)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 100)               50100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_17 (LeakyReLU)   (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 1,063,677\n",
            "Trainable params: 1,060,909\n",
            "Non-trainable params: 2,768\n",
            "_________________________________________________________________\n",
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_10 (InputLayer)        [(None, 100)]             0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_18 (LeakyReLU)   (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 500)               50500     \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_19 (LeakyReLU)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 784)               392784    \n",
            "_________________________________________________________________\n",
            "reshape_3 (Reshape)          (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 455,784\n",
            "Trainable params: 454,584\n",
            "Non-trainable params: 1,200\n",
            "_________________________________________________________________\n",
            "Model: \"gan\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_11 (InputLayer)        [(None, 100, 1)]          0         \n",
            "_________________________________________________________________\n",
            "generator (Functional)       (None, 28, 28, 1)         455784    \n",
            "_________________________________________________________________\n",
            "discriminator (Functional)   (None, 1)                 1063677   \n",
            "=================================================================\n",
            "Total params: 1,519,461\n",
            "Trainable params: 454,584\n",
            "Non-trainable params: 1,064,877\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdK01WT7hlwy"
      },
      "source": [
        "# TODO: Generate some images w/ trained generator"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}